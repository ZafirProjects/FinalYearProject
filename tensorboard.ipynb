{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import layers, Sequential, models\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Sequential([\n",
    "            layers.Dense(115, activation=\"relu\"),\n",
    "            layers.Dense(86, activation=\"relu\"),\n",
    "            layers.Dense(57, activation=\"relu\"),\n",
    "            layers.Dense(37, activation=\"relu\"),\n",
    "            layers.Dense(28, activation=\"relu\")\n",
    "        ])\n",
    "        self.decoder = Sequential([\n",
    "            layers.Dense(37, activation=\"relu\"),\n",
    "            layers.Dense(57, activation=\"relu\"),\n",
    "            layers.Dense(86, activation=\"relu\"),\n",
    "            layers.Dense(115, activation=\"sigmoid\")\n",
    "        ])\n",
    "    \n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "for i in range(9):\n",
    "    # create a training set using the benign dataset\n",
    "    benign = np.loadtxt(f'dataset/{i+1}.benign.csv', delimiter=\",\", skiprows=1)\n",
    "    X_train = benign[:40000]\n",
    "    # run 10 times to get an average\n",
    "    for ii in range(10):\n",
    "        x = scaler.fit_transform(X_train)\n",
    "        \n",
    "        # create an autoencoder model\n",
    "        ae = Autoencoder()\n",
    "        ae.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "        monitor = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=1e-9,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode='auto'\n",
    "        )\n",
    "        \n",
    "        log_dir = f\"logs/fit/deep/device{i+1}/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        ae.fit(\n",
    "            x=x,\n",
    "            y=x,\n",
    "            epochs=800,\n",
    "            validation_split=0.3,\n",
    "            shuffle=True,\n",
    "            callbacks=[monitor, tensorboard_callback]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "def train(ae, stage):\n",
    "    for iii in range(3):\n",
    "        # create a training set using the benign dataset\n",
    "        benign = np.loadtxt(f'dataset/{iii+1}.benign.csv', delimiter=\",\", skiprows=1)\n",
    "        X_train = benign[:40000]\n",
    "        # run 10 times to get an average\n",
    "        x = scaler.fit_transform(X_train)\n",
    "            \n",
    "        # create an autoencoder model\n",
    "        ae.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\n",
    "        monitor = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=1e-9,\n",
    "            patience=5,\n",
    "            verbose=1,\n",
    "            mode='auto'\n",
    "        )\n",
    "        \n",
    "        log_dir = f\"logs/fit/{stage}/device{i+1}/device{iii+1}/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        \n",
    "        ae.fit(\n",
    "            x=x,\n",
    "            y=x,\n",
    "            epochs=800,\n",
    "            validation_split=0.3,\n",
    "            shuffle=True,\n",
    "            callbacks=[monitor, tensorboard_callback]\n",
    "        )\n",
    "\n",
    "def tlencoder(ae):\n",
    "    if ii == 0 and i == 0:\n",
    "        ae.get_layer('sequential').get_layer('dense').trainable = False\n",
    "        ae.get_layer('sequential').get_layer('dense_1').trainable = False\n",
    "        ae.get_layer('sequential').get_layer('dense_2').trainable = False\n",
    "    else:\n",
    "        ae.get_layer(f'sequential_{(i*20)+(2*ii)}').get_layer(f'dense_{(90*i)+(9*ii)}').trainable = False\n",
    "        ae.get_layer(f'sequential_{(i*20)+(2*ii)}').get_layer(f'dense_{(90*i)+(9*ii)+1}').trainable = False\n",
    "        ae.get_layer(f'sequential_{(i*20)+(2*ii)}').get_layer(f'dense_{(90*i)+(9*ii)+2}').trainable = False\n",
    "    train(ae, \"encodertl\")\n",
    "\n",
    "def tlbottleneck(ae):\n",
    "    if ii == 0 and i == 0:\n",
    "        ae.get_layer('sequential').get_layer('dense_3').trainable = False\n",
    "        ae.get_layer('sequential').get_layer('dense_4').trainable = False\n",
    "    else:\n",
    "        ae.get_layer(f'sequential_{(i*20)+(2*ii)}').get_layer(f'dense_{(90*i)+(9*ii)+3}').trainable = False\n",
    "        ae.get_layer(f'sequential_{(i*20)+(2*ii)}').get_layer(f'dense_{(90*i)+(9*ii)+4}').trainable = False\n",
    "        ae.get_layer(f'sequential_{(i*20)+(2*ii)+1}').get_layer(f'dense_{(90*i)+(9*ii)+5}').trainable = False\n",
    "    train(ae, \"bottlenecktl\")\n",
    "    \n",
    "def tldecoder(ae):\n",
    "    ae.get_layer(f'sequential_{(i*20)+(2*ii)+1}').get_layer(f'dense_{(90*i)+(9*ii)+6}').trainable = False\n",
    "    ae.get_layer(f'sequential_{(i*20)+(2*ii)+1}').get_layer(f'dense_{(90*i)+(9*ii)+7}').trainable = False\n",
    "    ae.get_layer(f'sequential_{(i*20)+(2*ii)+1}').get_layer(f'dense_{(90*i)+(9*ii)+8}').trainable = False\n",
    "    train(ae, \"decodertl\")\n",
    "\n",
    "for i in range(3):\n",
    "    for ii in range(5):\n",
    "        ae = models.load_model(f\"neuralnetworks/deep/device{i+1}/run{ii+1}\")\n",
    "        tlencoder(ae)\n",
    "        ae = models.load_model(f\"neuralnetworks/deep/device{i+1}/run{ii+1}\")\n",
    "        tlbottleneck(ae)\n",
    "        ae = models.load_model(f\"neuralnetworks/deep/device{i+1}/run{ii+1}\")\n",
    "        tldecoder(ae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit/device2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
